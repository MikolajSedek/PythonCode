{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Szybka eksploracja danych i budowanie modeli predykcyjnych z użyciem Pythona\n",
    "\n",
    "#### Autor: Mikołaj Sędek, Data Scientist, Grupa Pracuj Sp. z.o.o.\n",
    "\n",
    "##### GIT: https://github.com/MikolajSedek/PythonCode/tree/master/DataScienceWBiznesie_7_12_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T14:09:50.954000",
     "start_time": "2016-11-19T14:09:50.946000"
    }
   },
   "source": [
    "<img src=\"img/img005.png\", width=400,height=400>\n",
    "<img src=\"img/img001.png\", width=400,height=400>\n",
    "<img src=\"img/img002.png\", width=400,height=400>\n",
    "<img src=\"img/img003.png\", width=400,height=400>\n",
    "<img src=\"img/img004.jpg\", width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Załadujmy moduły potrzebne nam do pracy\n",
    "\n",
    "##### Uwaga: instalacja modułów xgboost i keras/theano w środowisku Windows jest czasochłonna i skomplikowana, w Linuxie jest o wiele łatwiej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:30:44.643000",
     "start_time": "2016-12-05T10:30:44.638000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:31:15.188000",
     "start_time": "2016-12-05T10:31:15.154000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import modułów potrzebnych do modelowania\n",
    "\n",
    "import xgboost\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import h5py\n",
    "import theano\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l1l2\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takie wersje modułów są zainstalowane na mojej maszynie:\n",
    "xgboost:0.4\n",
    "sklearn:0.18\n",
    "pandas:0.19.1\n",
    "keras:1.1.0\n",
    "theano: 0.8.2\n",
    "seaborn:0.7.1\n",
    "matplotlib:1.5.1\n",
    "imblearn:0.1.8\n",
    "h5py:2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:31:50.491000",
     "start_time": "2016-12-05T10:31:50.482000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdzenie wersji modułów\n",
    "\n",
    "print(\"xgboost:\"+str(xgboost.__version__))\n",
    "print(\"sklearn:\"+str(sklearn.__version__))\n",
    "print(\"pandas:\"+str(pd.__version__))\n",
    "print(\"keras:\"+str(keras.__version__))\n",
    "print(\"theano:\"+str(theano.__version__))\n",
    "print(\"seaborn:\"+str(sns.__version__))\n",
    "print(\"matplotlib:\"+str(matplotlib.__version__))\n",
    "print(\"imblearn:\"+str(imblearn.__version__))\n",
    "print(\"h5py:\"+str(h5py.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie danych zajmie ok. 30 sekund - to ponad 300 tys. rekordów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:32:19.519000",
     "start_time": "2016-12-05T10:31:57.866000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ładujemy dane z plików csv\n",
    "\n",
    "dataset_learn = pd.read_csv(\"datasets\\data_learning.csv\", sep=\";\", decimal=\".\", compression=\"gzip\",index_col=False)\n",
    "\n",
    "dataset_apply = pd.read_csv(\"datasets\\data_apply.csv\", sep=\";\", decimal=\".\", compression=\"gzip\",index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:32:21.837000",
     "start_time": "2016-12-05T10:32:21.831000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zobaczmy z jakimi danymi mamy do czynienia\n",
    "\n",
    "print(\"dane uczące - wiersze,kolumny: \" + str(dataset_learn.shape))\n",
    "\n",
    "print(\"dane wdrożeniowe - wiersze,kolumny: \" + str(dataset_apply.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:32:27.150000",
     "start_time": "2016-12-05T10:32:27.144000"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#zmienne w zbiorze uczącym i wdrożeniowym\n",
    "\n",
    "dl_vars= list(dataset_learn.columns.values)\n",
    "da_vars= list(dataset_apply.columns.values)\n",
    "\n",
    "\n",
    "print(dl_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:32:28.215000",
     "start_time": "2016-12-05T10:32:28.210000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(da_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zobaczmy jak wygląda ramka danych na których będziemy pracować"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:32:38.939000",
     "start_time": "2016-12-05T10:32:38.914000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_learn.iloc[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:32:47.888000",
     "start_time": "2016-12-05T10:32:47.862000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_learn.iloc[0:10,380:390]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdźmy częstości w zmiennej celu - może się okazać że będzie trzeba będzie zbalansować dane uczące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:35:01.749000",
     "start_time": "2016-12-05T10:35:01.526000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_learn.groupby(\"Target\").size().plot(kind=\"bar\",\n",
    "                                            title=\"Zmienna celu - zliczenia\", colormap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:35:07.019000",
     "start_time": "2016-12-05T10:35:06.980000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataset_learn.groupby(\"Target\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiele modeli preferuje zmienną celu w formie binarnej - 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:35:32.267000",
     "start_time": "2016-12-05T10:35:30.742000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#przekodowanie zmiennej celu na zmienną binarną\n",
    "\n",
    "#skorzystamy z fukncji dostępnej w sklearn\n",
    "le = LabelEncoder()\n",
    "le.fit(dataset_learn[\"Target\"])\n",
    "\n",
    "print(\"oryginalne klasy w zmiennej celu: \" +str(le.classes_))\n",
    "Target_New = le.transform(dataset_learn[\"Target\"])\n",
    "\n",
    "dataset_learn[\"Target\"]=Target_New\n",
    "dataset_learn.groupby(\"Target\").size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadbajmy o braki danych oraz usunięcie zmiennych z zerową wariancją"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:35:42.759000",
     "start_time": "2016-12-05T10:35:40.030000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usuwamy braki danych\n",
    "\n",
    "#znajdźmy zmienne z brakami danych\n",
    "dict_miss = {k:v for (k,v) in dict(dataset_learn.isnull().sum()).items() if v > 1}\n",
    "\n",
    "pd.DataFrame.from_dict(dict_miss,orient=\"index\").plot(kind=\"bar\",legend=False,\n",
    "                                                      title=\"Zmienne z brakami danych\",\n",
    "                                                      ylim=(0,dataset_learn.shape[0]),\n",
    "                                                     colormap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:36:33.800000",
     "start_time": "2016-12-05T10:36:31.178000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wypełnienie braków danych medianą\n",
    "miss_vars = dict_miss.keys()\n",
    "\n",
    "dataset_learn[miss_vars] = dataset_learn[miss_vars].fillna(dataset_learn[miss_vars].median())\n",
    "\n",
    "dict_miss_check = {k:v for (k,v) in dict(dataset_learn.isnull().sum()).items() if v > 1}\n",
    "\n",
    "print(\"lista zmiennych w których nadal występują braki danych: \"+ str(dict_miss_check))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:36:43.420000",
     "start_time": "2016-12-05T10:36:40.416000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usuwamy zmienne mające zerową wariancję (stałych :) )\n",
    "  \n",
    "\n",
    "vars_unique = pd.DataFrame(dataset_learn.apply(pd.Series.nunique))\n",
    "\n",
    "vars_constant = list(vars_unique[vars_unique[0]==1].index)\n",
    "\n",
    "print(\"zmienne w których występuje zerowa wariancja:\")\n",
    "print(str(vars_constant))\n",
    "\n",
    "print(dataset_learn[vars_constant].head)\n",
    "\n",
    "\n",
    "dataset_learn.drop(vars_constant,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podzielmy nasze dane na zbiór uczący i testowy, zbalansujmy dane uczące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:36:59.220000",
     "start_time": "2016-12-05T10:36:56.998000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#podział na zbiór testowy i uczący\n",
    "\n",
    "seed = 7\n",
    "\n",
    "training, testing = train_test_split(dataset_learn, test_size = 0.2, random_state=seed)\n",
    "\n",
    "print(\"wielkość zbioru uczącego :\"+ str(training.shape))\n",
    "print(\"wielkość zbioru testowego :\"+ str(testing.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balansowanie zbioru uczącego można wykonać za pomocą modułu imblern \n",
    "\n",
    "dla osób zainteresowanych tematyką niezbalansowanych zbiorów uczących:<br>\n",
    "http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ <br>\n",
    "https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set <br>\n",
    "https://github.com/scikit-learn-contrib/imbalanced-learn <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:37:08.495000",
     "start_time": "2016-12-05T10:37:05.906000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zbalansowanie zbioru uczącego\n",
    "\n",
    "iv_vars = list(training.columns.values)\n",
    "iv_vars.remove(\"Target\")\n",
    "\n",
    "#definicja metody próbkowania\n",
    "rus = RandomUnderSampler(replacement=False, random_state=seed)\n",
    "\n",
    "training_rs_ivs, training_rs_dv = rus.fit_sample(training[iv_vars], training[\"Target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:37:11.525000",
     "start_time": "2016-12-05T10:37:11.485000"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"liczba zer: \"+str(list(training_rs_dv).count(0)))\n",
    "print(\"liczba jedynek: \"+str(list(training_rs_dv).count(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:37:14.841000",
     "start_time": "2016-12-05T10:37:14.548000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tworzymy nową ramkę danych ze zbalansowanymi danymi treningowymi\n",
    "\n",
    "training_bal = pd.DataFrame(training_rs_ivs)\n",
    "training_bal.columns = iv_vars\n",
    "training_bal[\"Target\"]=training_rs_dv\n",
    "training_bal=training_bal.sample(frac=1)\n",
    "\n",
    "training_bal.iloc[0:10,0:10]\n",
    "\n",
    "training_bal.groupby(\"Target\").size().plot(kind=\"bar\",\n",
    "                                           title=\"Zbalansowana zmienna celu w nowej ramce danych\",\n",
    "                                          colormap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bardzo ważnym etapem procesu jest dobór optymalnych predyktorów\n",
    "#### Ponieważ korzystamy z anonimowych zmiennych zastosujemy statystykę ważności zmiennych z modelu Random Forest (analiza wrażliwości)\n",
    "#### http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:39:23.507000",
     "start_time": "2016-12-05T10:38:52.604000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dobór zmiennych z użyciem Random Forest \n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=10, min_samples_split=20, min_samples_leaf=10, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=True, n_jobs=-1, random_state=seed, verbose=1, warm_start=False, class_weight=None)\n",
    "\n",
    "rf.fit(training_bal[iv_vars],training_bal[\"Target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:39:32.496000",
     "start_time": "2016-12-05T10:39:31.614000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#selekcja najlepszych zmiennych dzięki statytyskom ważności z Random Forest\n",
    "\n",
    "features_frame = pd.DataFrame()\n",
    "\n",
    "features_frame[\"vars\"]=iv_vars\n",
    "features_frame[\"importance\"]=rf.feature_importances_\n",
    "\n",
    "features_frame.sort_values(by=\"importance\",ascending=False,inplace=True)\n",
    "\n",
    "features_frame[features_frame.importance>0].plot(kind=\"area\",use_index=False, colormap=\"coolwarm\")\n",
    "\n",
    "features_frame.iloc[0:30].plot(kind=\"bar\",x=\"vars\",y=\"importance\", colormap=\"coolwarm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:40:05.480000",
     "start_time": "2016-12-05T10:40:05.471000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"zmienne o ważności większej niż 1%: \"+ str(features_frame[features_frame.importance>0.01].shape[0]))\n",
    "\n",
    "imp_vars = list(features_frame[features_frame.importance>0.01].vars)\n",
    "\n",
    "print(imp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:40:24.580000",
     "start_time": "2016-12-05T10:40:15.932000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zbadajmy przy okazji korelacje między predyktorami - np. 5 najważniejszymi wg rankingu Random Forest\n",
    "#przy okazji może odryjemy coś ciekawego związanego z rozkładami zmiennych\n",
    "\n",
    "\n",
    "top5vars = list(features_frame.vars[0:5])\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "sns.pairplot(training_bal[top5vars].sample(frac=0.5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:41:04.617000",
     "start_time": "2016-12-05T10:41:00.888000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#może zbadamy relacje między dwiema zmiennymi bardziej dokładnie - na próbce danych\n",
    "\n",
    "sns.jointplot(\"v110\", \"v17\", data=training_bal.sample(frac=0.5), kind=\"reg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozpoczynamy budowanie modeli predykcyjnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początek klasyczny model statystyczny - regresja logistyczna, zobaczymy jak poradzi sobie z naszymi danymi. Potem zastosujemy \n",
    "dużo potężniejsze uczenie maszynowe :).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:42:55.348000",
     "start_time": "2016-12-05T10:42:54.354000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#będziemy chcieli \"poprawić\" dystrybucje zmiennych dzięki przekształceniu logarytmicznemu log(1+wartość zmiennej),\n",
    "#upewnijmy się że w naszych zmiennych nie ma wartości ujemnych\n",
    "\n",
    "training_bal_neg = training_bal.columns[(training_bal < 0).any()].tolist()\n",
    "testing_neg = testing.columns[(testing < 0).any()].tolist()\n",
    "\n",
    "if len(training_bal_neg)>0:\n",
    "    for i in training_bal_neg:\n",
    "        training_bal[i]=np.where(training_bal[i]<0,0,training_bal[i])\n",
    "\n",
    "if len(testing_neg)>0:\n",
    "    for i in testing_neg:\n",
    "        testing[i]=np.where(testing[i]<0,0,testing[i])\n",
    "\n",
    "print(\"zmienne z wartościami ujemnymi w zbiorze uczącym: \"+ str(training_bal_neg))        \n",
    "print(\"zmienne z wartościami ujemnymi w zbiorze testowym: \"+ str(testing_neg))        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stworzymy sobie własną funkcję transformującą - skorzystamy z funkcji numpy log1p i przetestujemy ją na danych\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.log1p.html <br>\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html#function-transformer <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:43:17.149000",
     "start_time": "2016-12-05T10:43:11.746000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "transformer_log1p = FunctionTransformer(np.log1p)\n",
    "\n",
    "sns.pairplot(training_bal[top5vars].apply(np.log1p, axis=0).sample(frac=0.2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:43:51.003000",
     "start_time": "2016-12-05T10:43:38.957000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regresja logistyczna z kroswalidacją i regularyzacją typu l2 (ridge regression)\n",
    "lr = LogisticRegressionCV(Cs=10, fit_intercept=True, cv=5, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=-1, verbose=1, refit=True, intercept_scaling=1.0, multi_class='ovr', random_state=seed)\n",
    "\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('log1p', transformer_log1p))\n",
    "estimators.append(('lr_model', lr))\n",
    "pipeline_lr = Pipeline(estimators)\n",
    "\n",
    "\n",
    "pipeline_lr.fit(training_bal[imp_vars],training_bal[\"Target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:44:13.220000",
     "start_time": "2016-12-05T10:44:13.088000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_lr= pipeline_lr.predict(testing[imp_vars])\n",
    "pred_lr_proba = pipeline_lr.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_lr_proba_1 = list(pd.DataFrame(pred_lr_proba)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:44:15.115000",
     "start_time": "2016-12-05T10:44:14.803000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#liczymy macierz błędu dla regresji logistycznej\n",
    "\n",
    "cf_lr=confusion_matrix(testing[\"Target\"],pred_lr)\n",
    "\n",
    "cf_lr_pd = pd.DataFrame(cf_lr)\n",
    "cf_lr_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_lr_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "\n",
    "\n",
    "cf_lr_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu regresji logistycznej\")\n",
    "\n",
    "sens = round(cf_lr[1][1]*1.00 / np.sum(cf_lr[1]),2)\n",
    "spec = round(cf_lr[0][0]*1.00 / np.sum(cf_lr[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_lr_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:44:23.511000",
     "start_time": "2016-12-05T10:44:23.259000"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spróbujmy to narysować :)\n",
    "\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(testing[\"Target\"], pred_lr_proba_1, pos_label=1, drop_intermediate = False)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label=u'Krzywa ROC reg.logistyczna (obszar pod krzywą = %0.2f)' % roc_auc_lr)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(u'1 - Specyficzność')\n",
    "plt.ylabel(u'Czułość')\n",
    "plt.title('Krzywa ROC dla regresji logistycznej')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czas na ciężką artylerię - Xtreeme Gradient Boosting Trees \n",
    "\n",
    "### https://xgboost.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:45:04.153000",
     "start_time": "2016-12-05T10:44:43.511000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_xgbm = XGBClassifier(base_score=0.5, colsample_bylevel=0.5, colsample_bytree=0.5,\n",
    "       gamma=0, learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, n_estimators=2000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1e-4,\n",
    "       scale_pos_weight=1, seed=seed, silent=False, subsample=0.5)\n",
    "\n",
    "#przygotowujemy dane do uczenia i testowania w formie macierzy - będzie szybciej :)\n",
    "training_X = training_bal[imp_vars].apply(np.log1p, axis=0).as_matrix()\n",
    "training_y = training_bal[\"Target\"].as_matrix()\n",
    "\n",
    "testing_X= testing[imp_vars].apply(np.log1p, axis=0).as_matrix()\n",
    "testing_y = testing[\"Target\"].as_matrix()\n",
    "\n",
    "\n",
    "model_xgbm.fit(training_X,training_y, early_stopping_rounds=20, eval_metric=\"auc\", eval_set=[(testing_X, testing_y)])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:45:31.363000",
     "start_time": "2016-12-05T10:45:30.306000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_xgb= model_xgbm.predict(testing_X)\n",
    "pred_xgb_proba = model_xgbm.predict_proba(testing_X)\n",
    "\n",
    "pred_xgb_proba_1 = list(pd.DataFrame(pred_xgb_proba)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:45:31.644000",
     "start_time": "2016-12-05T10:45:31.367000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#liczymy macierz błędu dla xgboost\n",
    "\n",
    "cf_xgb=confusion_matrix(testing[\"Target\"],pred_xgb)\n",
    "\n",
    "cf_xgb_pd = pd.DataFrame(cf_xgb)\n",
    "cf_xgb_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_xgb_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "\n",
    "\n",
    "cf_xgb_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu xgboost\")\n",
    "\n",
    "\n",
    "sens = round(cf_xgb[1][1]*1.00 / np.sum(cf_xgb[1]),2)\n",
    "spec = round(cf_xgb[0][0]*1.00 / np.sum(cf_xgb[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_xgb_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:45:37.375000",
     "start_time": "2016-12-05T10:45:37.342000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(testing[\"Target\"], pred_xgb_proba_1, pos_label=1, drop_intermediate = False)\n",
    "\n",
    "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:45:38.422000",
     "start_time": "2016-12-05T10:45:38.166000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label=u'Krzywa ROC reg.logistyczna (obszar pod krzywą = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label=u'Krzywa ROC xgboost (obszar pod krzywą = %0.2f)' % roc_auc_xgb, linestyle='--')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(u'1 - Specyficzność')\n",
    "plt.ylabel(u'Czułość')\n",
    "plt.title('Krzywa ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spróbujmy jeszcze z Support Vector Machines\n",
    "\n",
    "### http://scikit-learn.org/stable/modules/svm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:46:21.799000",
     "start_time": "2016-12-05T10:46:13.558000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_svm = training_bal.sample(n=10000,replace=False)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#konfigurujemy model z użyciem baggingu\n",
    "n_estimators = 10\n",
    "model_svm = BaggingClassifier(SVC(kernel=\"linear\", C=3, gamma='auto', max_iter=-1, probability=True, random_state=seed, shrinking=True, verbose=True, tol=1e-5), max_samples=1.0 / n_estimators, n_estimators=n_estimators, oob_score=True, random_state=seed)\n",
    "\n",
    "#dodajmy standaryzację - model svm tak jak sieć neuronowa może być na to wrażliwy\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('svm', model_svm))\n",
    "pipeline_svm = Pipeline(estimators)\n",
    "\n",
    "\n",
    "\n",
    "#calculate model\n",
    "pipeline_svm.fit(training_svm[imp_vars],training_svm[\"Target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:47:37.824000",
     "start_time": "2016-12-05T10:47:21.792000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_svm= pipeline_svm.predict(testing[imp_vars])\n",
    "pred_svm_proba = pipeline_svm.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_svm_proba_1 = list(pd.DataFrame(pred_svm_proba)[1])\n",
    "\n",
    "#liczymy macierz błędu dla svm\n",
    "\n",
    "cf_svm=confusion_matrix(testing[\"Target\"],pred_svm)\n",
    "\n",
    "cf_svm_pd = pd.DataFrame(cf_svm)\n",
    "cf_svm_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_svm_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "cf_svm_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu svm\")\n",
    "\n",
    "sens = round(cf_svm[1][1]*1.00 / np.sum(cf_svm[1]),2)\n",
    "spec = round(cf_svm[0][0]*1.00 / np.sum(cf_svm[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_svm_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:47:42.584000",
     "start_time": "2016-12-05T10:47:42.552000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(testing[\"Target\"], pred_svm_proba_1, pos_label=1, drop_intermediate = False)\n",
    "\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:47:43.269000",
     "start_time": "2016-12-05T10:47:42.984000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label=u'Krzywa ROC reg.logistyczna (obszar pod krzywą = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label=u'Krzywa ROC xgboost (obszar pod krzywą = %0.2f)' % roc_auc_xgb, linestyle='--')\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkviolet',\n",
    "         lw=lw, label=u'Krzywa ROC svm bagging (obszar pod krzywą = %0.2f)' % roc_auc_svm, linestyle=':')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(u'1 - Specyficzność')\n",
    "plt.ylabel(u'Czułość')\n",
    "plt.title('Krzywe ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A teraz spróbujmy podejścia z sieciami neuronowymi\n",
    "\n",
    "### https://keras.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:50:33.229000",
     "start_time": "2016-12-05T10:50:33.217000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dajmy kerasowi pełną moc na CPU \n",
    "\n",
    "#multithreading\n",
    "import multiprocessing\n",
    "num_cores = str(multiprocessing.cpu_count())\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = num_cores\n",
    "os.environ['GOTO_NUM_THREADS'] = num_cores\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = num_cores\n",
    "\n",
    "import mkl\n",
    "mkl.set_num_threads(int(num_cores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:50:40.818000",
     "start_time": "2016-12-05T10:50:40.802000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#skorzystamy z wrappera KerasClassifier który pozwoli nam skorzystać z modelu Keras/Theano w ScikitLearn\n",
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(len(imp_vars),)))\n",
    "    model.add(Dense(100, init='normal', activation='relu', W_constraint=maxnorm(2), W_regularizer=l1l2(l1=0,l2=1e-4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(60, init='normal', activation='relu', W_constraint=maxnorm(2), W_regularizer=l1l2(l1=0,l2=1e-4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.005, momentum=0.9, decay=0.0, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['fbeta_score'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:54:53.562000",
     "start_time": "2016-12-05T10:54:53.485000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dodajemy możliwość zatrzymania nauki modelu gdy przez 5 kolejnych epok wyniki się nie poprawią - na podstawie statystyk z danych walidacyjnych\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_fbeta_score', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "#zapamiętujemy najlepszy model w pliku - uwaga! konieczna instalacja biblioteki h5py\n",
    "filepath=\"temp\\model_best.hdf5\"\n",
    "modelCheck = keras.callbacks.ModelCheckpoint(filepath, monitor='val_fbeta_score', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "#przygotowujemy dane do uczenia i testowania w formie macierzy - będzie szybciej :)\n",
    "training_X = training_bal[imp_vars].as_matrix()\n",
    "training_y = training_bal[\"Target\"]\n",
    "\n",
    "testing_X= testing[imp_vars].as_matrix()\n",
    "testing_y = testing[\"Target\"].as_matrix()\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('log1p', transformer_log1p))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, nb_epoch=20,batch_size=100, verbose=1, validation_data=(testing_X, testing_y), callbacks=[earlyStopping,modelCheck])))\n",
    "pipeline_keras_nnet = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:55:37.902000",
     "start_time": "2016-12-05T10:54:55.274000"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uczymy model\n",
    "\n",
    "pipeline_keras_nnet.fit(training_X,training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:56:20.321000",
     "start_time": "2016-12-05T10:56:17.545000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_nnet= pipeline_keras_nnet.predict(testing[imp_vars])\n",
    "pred_nnet_proba = pipeline_keras_nnet.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_nnet_proba_1 = list(pd.DataFrame(pred_nnet_proba)[1])\n",
    "\n",
    "\n",
    "#liczymy macierz błędu dla nnet\n",
    "\n",
    "cf_nnet=confusion_matrix(testing[\"Target\"],pred_nnet)\n",
    "\n",
    "cf_nnet_pd = pd.DataFrame(cf_nnet)\n",
    "cf_nnet_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_nnet_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "cf_nnet_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu nnet (Keras/Theano)\")\n",
    "\n",
    "sens = round(cf_nnet[1][1]*1.00 / np.sum(cf_nnet[1]),2)\n",
    "spec = round(cf_nnet[0][0]*1.00 / np.sum(cf_nnet[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_nnet_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:56:24.203000",
     "start_time": "2016-12-05T10:56:24.173000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_nnet, tpr_nnet, thresholds_nnet = roc_curve(testing[\"Target\"], pred_nnet_proba_1, pos_label=1, drop_intermediate = False)\n",
    "\n",
    "roc_auc_nnet = auc(fpr_nnet, tpr_nnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:56:25.304000",
     "start_time": "2016-12-05T10:56:25.050000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkviolet',\n",
    "         lw=lw, label=u'Krzywa ROC svm (obszar pod krzywą = %0.2f)' % roc_auc_svm, linestyle=':')\n",
    "plt.plot(fpr_nnet, tpr_nnet, color='darkblue',\n",
    "         lw=lw, label=u'Krzywa ROC nnet (obszar pod krzywą = %0.2f)' % roc_auc_nnet, linestyle=':')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(u'1 - Specyficzność')\n",
    "plt.ylabel(u'Czułość')\n",
    "plt.title('Krzywe ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Można skorzystać z kilku modeli na raz :) \n",
    "\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:59:08.426000",
     "start_time": "2016-12-05T10:59:08.419000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#krok1 - definiujemy modele\n",
    "\n",
    "clf1 = LogisticRegressionCV(Cs=10, fit_intercept=True, cv=5, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=-1, verbose=1, refit=True, intercept_scaling=1.0, multi_class='ovr', random_state=seed)\n",
    "\n",
    "clf2 = ExtraTreesClassifier(n_estimators=500, criterion='gini', max_depth=10, min_samples_split=20, min_samples_leaf=15, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=True, n_jobs=-1, random_state=seed, verbose=1, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:59:20.540000",
     "start_time": "2016-12-05T10:59:20.536000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#krok 2 - korzystamy z VotingClassifier w sklearn\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1),('xtrees',clf2)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T10:59:22.532000",
     "start_time": "2016-12-05T10:59:22.527000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators_eclf = []\n",
    "estimators_eclf.append(('log1p', transformer_log1p))\n",
    "estimators_eclf.append(('eclf', eclf))\n",
    "pipeline_eclf = Pipeline(estimators_eclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:00:02.982000",
     "start_time": "2016-12-05T10:59:35.550000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_eclf.fit(training_X,training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:00:07.895000",
     "start_time": "2016-12-05T11:00:02.986000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_eclf= pipeline_eclf.predict(testing[imp_vars])\n",
    "pred_eclf_proba = pipeline_eclf.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_eclf_proba_1 = list(pd.DataFrame(pred_eclf_proba)[1])\n",
    "\n",
    "#liczymy macierz błędu dla ensemble\n",
    "\n",
    "cf_eclf=confusion_matrix(testing[\"Target\"],pred_eclf)\n",
    "\n",
    "cf_eclf_pd = pd.DataFrame(cf_eclf)\n",
    "cf_eclf_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_eclf_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "cf_eclf_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu Ensemble\")\n",
    "\n",
    "sens = round(cf_eclf[1][1]*1.00 / np.sum(cf_eclf[1]),2)\n",
    "spec = round(cf_eclf[0][0]*1.00 / np.sum(cf_eclf[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_eclf_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:00:14.047000",
     "start_time": "2016-12-05T11:00:14.014000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr_eclf, tpr_eclf, thresholds_eclf = roc_curve(testing[\"Target\"], pred_eclf_proba_1, pos_label=1)\n",
    "roc_auc_eclf = auc(fpr_eclf, tpr_eclf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:00:15.007000",
     "start_time": "2016-12-05T11:00:14.721000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_nnet, tpr_nnet, color='darkgreen',\n",
    "         lw=lw, label=u'Krzywa ROC nnet (obszar pod krzywą = %0.2f)' % roc_auc_nnet, linestyle=':')\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label=u'Krzywa ROC xgboost (obszar pod krzywą = %0.2f)' % roc_auc_xgb, linestyle='-.')\n",
    "plt.plot(fpr_eclf, tpr_eclf, color='darkblue',\n",
    "         lw=lw, label=u'Krzywa ROC Ensemble (obszar pod krzywą = %0.2f)' % roc_auc_eclf, linestyle='--')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(u'1 - Specyficzność')\n",
    "plt.ylabel(u'Czułość')\n",
    "plt.title('Krzywe ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdźmy na koniec korelacje między wynikami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:00:30.003000",
     "start_time": "2016-12-05T11:00:29.908000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_frame = pd.DataFrame()\n",
    "\n",
    "pred_frame[\"ensemble\"] = list(pred_eclf_proba_1)\n",
    "pred_frame[\"xgboost\"] = list(pred_xgb_proba_1)\n",
    "pred_frame[\"nnet\"] = list(pred_nnet_proba_1)\n",
    "\n",
    "print(\"korelacje między modelami predykcyjnymi:\")\n",
    "print(str(pred_frame.corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:00:37.885000",
     "start_time": "2016-12-05T11:00:33.165000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(pred_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdźmy dystrybucję realnych 0 i 1 wobec zaokrąglonego prawdopodobieństwa 1 wg modelu xgboost\n",
    "\n",
    "##### Uwaga: jeśli chcemy znormalizować rozkład prawdopodobieństw nie tracąc dokładności modelu można pomyśleć o regresji izotonicznej: http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:01:03.921000",
     "start_time": "2016-12-05T11:01:03.456000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_stats = pd.DataFrame()\n",
    "\n",
    "pred_stats[\"Target\"]=testing[\"Target\"]\n",
    "pred_stats[\"proba1\"]=[round(x,1) for x in pred_xgb_proba_1]\n",
    "pred_stats[\"liczba\"]=1\n",
    "\n",
    "pred_stats_show= pred_stats.groupby(['Target', 'proba1'], as_index=False).count()\n",
    "\n",
    "sns.barplot(x=\"proba1\", y=\"liczba\", hue=\"Target\", data=pred_stats_show)                                  \n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przy selekcji finalnego modelu możemy też skorzystać z krzywej Precyzja-Czułość (Precision-Recall Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:01:15.611000",
     "start_time": "2016-12-05T11:01:15.192000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision_xgb, recall_xgb, thresholds_xgb = precision_recall_curve(y_true=testing[\"Target\"],probas_pred=pred_xgb_proba_1, pos_label=1)\n",
    "average_precision_xgb  = average_precision_score(testing[\"Target\"],pred_xgb_proba_1)\n",
    "\n",
    "precision_nnet, recall_nnet, thresholds_nnet = precision_recall_curve(y_true=testing[\"Target\"],probas_pred= pred_nnet_proba_1, pos_label=1)\n",
    "average_precision_nnet  = average_precision_score(testing[\"Target\"],pred_nnet_proba_1)\n",
    "\n",
    "precision_svm, recall_svm, thresholds_svm = precision_recall_curve(y_true=testing[\"Target\"],probas_pred= pred_svm_proba_1, pos_label=1)\n",
    "average_precision_svm  = average_precision_score(testing[\"Target\"],pred_svm_proba_1)\n",
    "\n",
    "lw=2\n",
    "plt.clf()\n",
    "plt.plot(recall_xgb, precision_xgb, lw=lw, color='red',\n",
    "         label=u'xgboost (średnia precyzja = %0.2f)' % average_precision_xgb, linestyle=':')\n",
    "plt.plot(recall_nnet, precision_nnet, lw=lw, color='darkblue',\n",
    "         label=u'nnet (średnia precyzja = %0.2f)' % average_precision_nnet, linestyle='-')\n",
    "plt.plot(recall_svm, precision_svm, lw=lw, color='darkgreen',\n",
    "         label=u'svm (średnia precyzja = %0.2f)' % average_precision_svm, linestyle='--')\n",
    "plt.xlabel(u'Czułość')\n",
    "plt.ylabel(u'Precyzja')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.title(u\"Krzywe Precyzja-Czułość dla wielu modeli\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wdrożenie najlepszych modeli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:02:51.644000",
     "start_time": "2016-12-05T11:02:51.589000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#przygotujmy dane\n",
    "\n",
    "dataset_apply[miss_vars] = dataset_apply[miss_vars].fillna(dataset_apply[miss_vars].median())\n",
    "\n",
    "apply_data = dataset_apply[imp_vars].as_matrix()\n",
    "\n",
    "print(\"dane wdrożeniowe: \"+str(apply_data.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:03:00.398000",
     "start_time": "2016-12-05T11:02:54.644000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wdrożenie modelu z keras i ensemle - przetworzenie danych przy użyciu log1p następuje dzięki pipeline\n",
    "\n",
    "pred_nnet_prob = pipeline_keras_nnet.predict_proba(apply_data)\n",
    "\n",
    "pred_ensemble_prob = pipeline_eclf.predict_proba(apply_data)\n",
    "\n",
    "#w przypapadku xgboost musimy dodatkowo użyć funkcji log1p\n",
    "apply_data_xgboost = dataset_apply[imp_vars].apply(np.log1p, axis=0).as_matrix()\n",
    "\n",
    "pred_xgboost_prob = model_xgbm.predict_proba(apply_data_xgboost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:03:05.666000",
     "start_time": "2016-12-05T11:03:05.627000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zbieramy dane do jednego DataFrame i zwizualizujemy\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "predictions[\"userid\"]=dataset_apply[\"userid\"]\n",
    "\n",
    "predictions[\"nnet\"]=pd.DataFrame(pred_nnet_prob)[1]\n",
    "predictions[\"ensemble\"]=pd.DataFrame(pred_ensemble_prob)[1]\n",
    "predictions[\"xgboost\"]=pd.DataFrame(pred_xgboost_prob)[1]\n",
    "\n",
    "#sprawdźmy dane\n",
    "predictions.iloc[1:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:03:16.230000",
     "start_time": "2016-12-05T11:03:09.576000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(predictions.iloc[:,1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:03:43.307000",
     "start_time": "2016-12-05T11:03:43.284000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"korelacje między predykcjami:\")\n",
    "print(str(predictions.iloc[:,1:4].corr()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T11:03:46.259000",
     "start_time": "2016-12-05T11:03:45.848000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zapis do pliku csv\n",
    "\n",
    "predictions.to_csv(\"results/predictions.csv\", index=False, header=True,sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dziękuję za udział w warsztatach, chętnie wysłucham Państwa uwag i pytań :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literatura warta uwagi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:17:24.936000",
     "start_time": "2016-11-20T15:17:24.929000"
    }
   },
   "source": [
    "<img src=\"img/book1.jpg\", width=400,height=400>\n",
    "<img src=\"img/book2.jpg\", width=400,height=400>\n",
    "<img src=\"img/book3.png\", width=400,height=400>\n",
    "<img src=\"img/book4.jpg\", width=400,height=400>\n",
    "<img src=\"img/book5.jpg\", width=400,height=400>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
