{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Szybka eksploracja danych i budowanie modeli predykcyjnych z użyciem Pythona\n",
    "\n",
    "### Autor: Mikołaj Sędek, Data Scientist, Grupa Pracuj Sp. z.o.o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T14:09:50.954000",
     "start_time": "2016-11-19T14:09:50.946000"
    }
   },
   "source": [
    "<img src=\"img005.png\", width=400,height=400>\n",
    "<img src=\"img001.png\", width=400,height=400>\n",
    "<img src=\"img002.png\", width=400,height=400>\n",
    "<img src=\"img003.png\", width=400,height=400>\n",
    "<img src=\"img004.jpg\", width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Załadujmy moduły potrzebne nam do pracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:39:46.322000",
     "start_time": "2016-11-20T15:39:46.290000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import modułów potrzebnych do modelowania\n",
    "\n",
    "import xgboost\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T13:13:08.781000",
     "start_time": "2016-11-20T13:13:08.775000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takie wersje modułów są zainstalowane na mojej maszynie:\n",
    "xgboost:0.4\n",
    "sklearn:0.18\n",
    "pandas:0.19.1\n",
    "keras:1.1.0\n",
    "seaborn:0.7.1\n",
    "matplotlib:1.5.1\n",
    "imblearn:0.1.8\n",
    "h5py:2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:22:08.596000",
     "start_time": "2016-11-20T15:22:08.587000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sprawdzenie wersji modułów\n",
    "\n",
    "print(\"xgboost:\"+str(xgboost.__version__))\n",
    "print(\"sklearn:\"+str(sklearn.__version__))\n",
    "print(\"pandas:\"+str(pd.__version__))\n",
    "print(\"keras:\"+str(keras.__version__))\n",
    "print(\"seaborn:\"+str(sns.__version__))\n",
    "print(\"matplotlib:\"+str(matplotlib.__version__))\n",
    "print(\"imblearn:\"+str(imblearn.__version__))\n",
    "print(\"h5py:\"+str(h5py.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie danych chwilę zajmie - to ponad 300 tys. rekordów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:23:58.904000",
     "start_time": "2016-11-20T15:23:36.332000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ładujemy dane z plików csv\n",
    "\n",
    "dataset_learn = pd.read_csv(\"data_learning.csv\", sep=\";\", decimal=\".\", compression=\"gzip\",index_col=False)\n",
    "\n",
    "dataset_apply = pd.read_csv(\"data_apply.csv\", sep=\";\", decimal=\".\", compression=\"gzip\",index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:23:58.915000",
     "start_time": "2016-11-20T15:23:58.907000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zobaczmy z jakimi danymi mamy do czynienia\n",
    "\n",
    "print(\"dane uczące - wiersze,kolumny: \" + str(dataset_learn.shape))\n",
    "\n",
    "print(\"dane wdrożeniowe - wiersze,kolumny: \" + str(dataset_apply.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:23:58.931000",
     "start_time": "2016-11-20T15:23:58.919000"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#zmienne które w zbiorze uczącym\n",
    "\n",
    "dl_vars= list(dataset_learn.columns.values)\n",
    "da_vars= list(dataset_apply.columns.values)\n",
    "\n",
    "\n",
    "print(dl_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:23:58.944000",
     "start_time": "2016-11-20T15:23:58.938000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(da_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zobaczmy jak wygląda ramka danych na których będziemy pracować"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:24:09.273000",
     "start_time": "2016-11-20T15:24:09.245000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_learn.iloc[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:24:12.147000",
     "start_time": "2016-11-20T15:24:12.120000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_learn.iloc[0:10,380:390]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdźmy częstości w zmiennej celu - może się okazać że będzie trzeba będzie zbalansować dane uczące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:25:06.602000",
     "start_time": "2016-11-20T15:25:06.388000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_learn.groupby(\"Target\").size().plot(kind=\"bar\",title=\"Zmienna celu - zliczenia\", colormap=\"bwr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T13:13:34.638000",
     "start_time": "2016-11-20T13:13:34.606000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataset_learn.groupby(\"Target\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiele modeli preferuje zmienną celu w formie binarnej - 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:29:42.988000",
     "start_time": "2016-11-20T15:29:42.938000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#przekodowanie zmiennej celu na zmienną binarną\n",
    "\n",
    "#skorzystamy z fukncji dostępnej w sklearn\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(dataset_learn[\"Target\"])\n",
    "\n",
    "print(le.classes_)\n",
    "\n",
    "\n",
    "Target_New = le.transform(dataset_learn[\"Target\"])\n",
    "\n",
    "dataset_learn[\"Target\"]=Target_New\n",
    "\n",
    "\n",
    "dataset_learn.groupby(\"Target\").size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:29:50.760000",
     "start_time": "2016-11-20T15:29:47.959000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usuwamy braki danych\n",
    "\n",
    "#znajdźmy zmienne z brakami danych\n",
    "dict_miss = {k:v for (k,v) in dict(dataset_learn.isnull().sum()).items() if v > 1}\n",
    "\n",
    "pd.DataFrame.from_dict(dict_miss,orient=\"index\").plot(kind=\"bar\",legend=False,title=\"Zmienne z brakami danych\",ylim=(0,dataset_learn.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadbajmy o braki danych oraz usunięcie zmiennych z zerową wariancją"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:29:57.488000",
     "start_time": "2016-11-20T15:29:54.774000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wypełnienie braków danych medianą\n",
    "miss_vars = dict_miss.keys()\n",
    "\n",
    "dataset_learn[miss_vars] = dataset_learn[miss_vars].fillna(dataset_learn[miss_vars].median())\n",
    "\n",
    "dict_miss_check = {k:v for (k,v) in dict(dataset_learn.isnull().sum()).items() if v > 1}\n",
    "\n",
    "print(\"lista zmiennych w których nadal występują braki danych: \"+ str(dict_miss_check))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:30:02.752000",
     "start_time": "2016-11-20T15:29:59.528000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usuwamy zmienne mające zerową wariancję (stałych :) )\n",
    "  \n",
    "\n",
    "vars_unique = pd.DataFrame(dataset_learn.apply(pd.Series.nunique))\n",
    "\n",
    "vars_constant = list(vars_unique[vars_unique[0]==1].index)\n",
    "\n",
    "print(\"zmienne w których występuje zerowa wariancja: \"+ str(vars_constant))\n",
    "\n",
    "print(dataset_learn[vars_constant].head)\n",
    "\n",
    "\n",
    "dataset_learn.drop(vars_constant,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podzielmy nasze dane na zbiór uczący i testowy, zbalansujmy dane uczące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:30:09.213000",
     "start_time": "2016-11-20T15:30:07.064000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#podział na zbiór testowy i uczący\n",
    "\n",
    "training, testing = train_test_split(dataset_learn, test_size = 0.2)\n",
    "\n",
    "print(\"wielkość zbioru uczącego :\"+ str(training.shape))\n",
    "print(\"wielkość zbioru testowego :\"+ str(testing.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balansowanie zbioru uczącego można wykonać za pomocą modułu imblern \n",
    "\n",
    "dla osób zainteresowanych tematyką niezbalansowanych zbiorów uczących:<br>\n",
    "http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ <br>\n",
    "https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set <br>\n",
    "https://github.com/scikit-learn-contrib/imbalanced-learn <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:30:15.082000",
     "start_time": "2016-11-20T15:30:12.252000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zbalansowanie zbioru uczącego\n",
    "\n",
    "iv_vars = list(training.columns.values)\n",
    "iv_vars.remove(\"Target\")\n",
    "\n",
    "#definicja metody próbkowania\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "\n",
    "training_rs_ivs, training_rs_dv = rus.fit_sample(training[iv_vars], training[\"Target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:30:17.881000",
     "start_time": "2016-11-20T15:30:17.842000"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"liczba zer: \"+str(list(training_rs_dv).count(0)))\n",
    "print(\"liczba jedynek: \"+str(list(training_rs_dv).count(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:30:22.347000",
     "start_time": "2016-11-20T15:30:22.010000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tworzymy nową ramkę danych ze zbalansowanymi danymi treningowymi\n",
    "\n",
    "training_bal = pd.DataFrame(training_rs_ivs)\n",
    "training_bal.columns = iv_vars\n",
    "training_bal[\"Target\"]=training_rs_dv\n",
    "training_bal=training_bal.sample(frac=1)\n",
    "\n",
    "training_bal.iloc[0:10,0:10]\n",
    "\n",
    "training_bal.groupby(\"Target\").size().plot(kind=\"bar\", title=\"Zbalansowana zmienna celu w nowej ramce danych\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bardzo ważnym etapem procesu jest dobór optymalnych predyktorów\n",
    "### Ponieważ korzystamy z anonimowych zmiennych zastosujemy statystykę ważności zmiennych z modelu Random Forest (analiza wrażliwości)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:33:38.522000",
     "start_time": "2016-11-20T15:33:06.746000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dobór zmiennych z użyciem Random Forest \n",
    "\n",
    "#szczegóły parametrów Random Forest: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=10, min_samples_split=20, min_samples_leaf=10, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=True, n_jobs=-1, random_state=None, verbose=1, warm_start=False, class_weight=None)\n",
    "\n",
    "rf.fit(training_bal[iv_vars],training_bal[\"Target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:33:45.459000",
     "start_time": "2016-11-20T15:33:44.552000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#selekcja najlepszych zmiennych dzięki statytyskom ważności z Random Forest\n",
    "\n",
    "features_frame = pd.DataFrame()\n",
    "\n",
    "features_frame[\"vars\"]=iv_vars\n",
    "features_frame[\"importance\"]=rf.feature_importances_\n",
    "\n",
    "features_frame.sort_values(by=\"importance\",ascending=False,inplace=True)\n",
    "\n",
    "features_frame[features_frame.importance>0].plot(kind=\"area\",use_index=False)\n",
    "\n",
    "features_frame.iloc[0:30].plot(kind=\"bar\",x=\"vars\",y=\"importance\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:34:29.126000",
     "start_time": "2016-11-20T15:34:29.116000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"liczba zmiennych o ważności większej niż 0.01: \"+ str(features_frame[features_frame.importance>0.01].shape[0]))\n",
    "\n",
    "imp_vars = list(features_frame[features_frame.importance>0.005].vars)\n",
    "\n",
    "print(imp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:34:43.022000",
     "start_time": "2016-11-20T15:34:33.550000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zbadajmy przy okazji korelacje między predyktorami - np. 5 najważniejszymi wg rankingu Random Forest\n",
    "#przy okazji może odryjemy coś ciekawego związanego z rozkładami zmiennych\n",
    "\n",
    "\n",
    "top5vars = list(features_frame.vars[0:5])\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "sns.pairplot(training_bal[top5vars].sample(frac=0.5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:35:16.086000",
     "start_time": "2016-11-20T15:35:12.268000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#może zbadamy relacje między dwiema zmiennymi bardziej dokładnie - na próbce danych\n",
    "\n",
    "sns.jointplot(\"v110\", \"v17\", data=training_bal.sample(frac=0.5), kind=\"reg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozpoczynamy budowanie modeli predykcyjnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początek klasyczny model statystyczny - regresja logistyczna, zobaczymy jak poradzi sobie z naszymi danymi. Potem zastosujemy \n",
    "dużo potężniejsze uczenie maszynowe :).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:36:00.377000",
     "start_time": "2016-11-20T15:35:52.593000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zastosujmy ustawienia domyślne poza zmianą verbose na 1 i n_jobs na -1 aby sklearn skorzystał ze wszystkich procesorów\n",
    "lr = LogisticRegression(penalty='l2', dual=False, tol=1e-8, C=1, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=1, warm_start=False, n_jobs=-1)\n",
    "\n",
    "lr.fit(training_bal[imp_vars],training_bal[\"Target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:36:12.447000",
     "start_time": "2016-11-20T15:36:12.369000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_lr= lr.predict(testing[imp_vars])\n",
    "pred_lr_proba = lr.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_lr_proba_1 = list(pd.DataFrame(pred_lr_proba)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:36:50.655000",
     "start_time": "2016-11-20T15:36:50.326000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#liczymy macierz błędu dla regresji logistycznej\n",
    "\n",
    "cf_lr=confusion_matrix(testing[\"Target\"],pred_lr)\n",
    "\n",
    "cf_lr_pd = pd.DataFrame(cf_lr)\n",
    "cf_lr_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_lr_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "\n",
    "\n",
    "cf_lr_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu regresji logistycznej\")\n",
    "\n",
    "sens = round(cf_lr[1][1]*1.00 / np.sum(cf_lr[1]),2)\n",
    "spec = round(cf_lr[0][0]*1.00 / np.sum(cf_lr[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_lr_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:37:21.306000",
     "start_time": "2016-11-20T15:37:21.048000"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spróbujmy to narysować :)\n",
    "\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(testing[\"Target\"], pred_lr_proba_1, pos_label=1)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label='Krzywa ROC reg.logistyczna (pod krzywa = %0.2f)' % roc_auc_lr)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Falszywe alarmy')\n",
    "plt.ylabel('Pozytywne wykrycia')\n",
    "plt.title('Krzywa ROC dla regresji logistycznej')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czas na ciężką artylerię - Xtreeme Gradient Boosting Trees \n",
    "\n",
    "### https://xgboost.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:40:50.585000",
     "start_time": "2016-11-20T15:40:38.142000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_xgbm = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=1e-4, reg_lambda=1e-4,\n",
    "       scale_pos_weight=1, seed=0, silent=False, subsample=0.5)\n",
    "\n",
    "model_xgbm.fit(training_bal[imp_vars],training_bal[\"Target\"], early_stopping_rounds=20, eval_metric=\"auc\", eval_set=[(testing[imp_vars], testing[\"Target\"])])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:41:13.641000",
     "start_time": "2016-11-20T15:41:13.069000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_xgb= model_xgbm.predict(testing[imp_vars])\n",
    "pred_xgb_proba = model_xgbm.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_xgb_proba_1 = list(pd.DataFrame(pred_xgb_proba)[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:41:21.209000",
     "start_time": "2016-11-20T15:41:20.902000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#liczymy macierz błędu dla xgboost\n",
    "\n",
    "cf_xgb=confusion_matrix(testing[\"Target\"],pred_xgb)\n",
    "\n",
    "cf_xgb_pd = pd.DataFrame(cf_xgb)\n",
    "cf_xgb_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_xgb_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "\n",
    "\n",
    "cf_xgb_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu xgboost\")\n",
    "\n",
    "\n",
    "sens = round(cf_xgb[1][1]*1.00 / np.sum(cf_xgb[1]),2)\n",
    "spec = round(cf_xgb[0][0]*1.00 / np.sum(cf_xgb[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_xgb_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:41:36.083000",
     "start_time": "2016-11-20T15:41:36.049000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(testing[\"Target\"], pred_xgb_proba_1, pos_label=1)\n",
    "\n",
    "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:41:36.958000",
     "start_time": "2016-11-20T15:41:36.736000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label='Krzywa ROC reg.logistyczna (pod krzywa = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label='Krzywa ROC xgboost (pod krzywa = %0.2f)' % roc_auc_xgb, linestyle='--')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Falszywe alarmy')\n",
    "plt.ylabel('Pozytywne wykrycia')\n",
    "plt.title('Krzywa ROC dla regresji logistycznej')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spróbujmy jeszcze z Support Vector Machines\n",
    "\n",
    "### http://scikit-learn.org/stable/modules/svm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:42:50.039000",
     "start_time": "2016-11-20T15:42:45.337000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_svm = training_bal.sample(n=10000,replace=False)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#konfigurujemy model z użyciem baggingu\n",
    "n_estimators = 5\n",
    "model_svm = BaggingClassifier(SVC(kernel=\"rbf\", gamma='auto', max_iter=-1, probability=True, random_state=None, shrinking=True, verbose=True, tol=1e-5), max_samples=1.0 / n_estimators, n_estimators=n_estimators)\n",
    "\n",
    "#dodajmy standaryzację - model svm tak jak sieć neuronowa może być na to wrażliwy\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('svm', model_svm))\n",
    "pipeline_svm = Pipeline(estimators)\n",
    "\n",
    "\n",
    "\n",
    "#calculate model\n",
    "pipeline_svm.fit(training_svm[imp_vars],training_svm[\"Target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:43:38.775000",
     "start_time": "2016-11-20T15:42:55.694000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_svm= pipeline_svm.predict(testing[imp_vars])\n",
    "pred_svm_proba = pipeline_svm.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_svm_proba_1 = list(pd.DataFrame(pred_svm_proba)[1])\n",
    "\n",
    "\n",
    "#liczymy macierz błędu dla svm\n",
    "\n",
    "cf_svm=confusion_matrix(testing[\"Target\"],pred_svm)\n",
    "\n",
    "cf_svm_pd = pd.DataFrame(cf_svm)\n",
    "cf_svm_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_svm_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "cf_svm_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu svm\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sens = round(cf_svm[1][1]*1.00 / np.sum(cf_svm[1]),2)\n",
    "spec = round(cf_svm[0][0]*1.00 / np.sum(cf_svm[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_svm_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:43:49.078000",
     "start_time": "2016-11-20T15:43:49.044000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(testing[\"Target\"], pred_svm_proba_1, pos_label=1)\n",
    "\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:43:50.716000",
     "start_time": "2016-11-20T15:43:50.471000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label='Krzywa ROC reg.logistyczna (pod krzywa = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label='Krzywa ROC xgboost (pod krzywa = %0.2f)' % roc_auc_xgb, linestyle='--')\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkviolet',\n",
    "         lw=lw, label='Krzywa ROC svm (pod krzywa = %0.2f)' % roc_auc_svm, linestyle=':')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Falszywe alarmy')\n",
    "plt.ylabel('Pozytywne wykrycia')\n",
    "plt.title('Krzywe ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A teraz spróbujmy podejścia z głębokimi sieciami neuronowymi\n",
    "\n",
    "### https://keras.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:46:05.122000",
     "start_time": "2016-11-20T15:46:05.110000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#skorzystamy z wrappera KerasClassifier który pozwoli nam skorzystać z modelu Keras/Theano w ScikitLearn\n",
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(len(imp_vars),)))\n",
    "    model.add(Dense(100, init='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(60, init='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.005, momentum=0.9, decay=0.0, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:46:05.776000",
     "start_time": "2016-11-20T15:46:05.697000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dodajemy możliwość zatrzymania nauki modelu gdy przez 5 kolejnych epok wyniki się nie poprawią - na podstawie statystyk z danych walidacyjnych\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "#zapamiętujemy najlepszy model w pliku - konieczna instalacja biblioteki h5py\n",
    "filepath=\"model_best.hdf5\"\n",
    "modelCheck = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "#przygotowujemy dane do uczenia i testowania w formie macierzy - będzie szybciej :)\n",
    "training_X = training_bal[imp_vars].as_matrix()\n",
    "training_y = training_bal[\"Target\"]\n",
    "\n",
    "testing_X= testing[imp_vars].as_matrix()\n",
    "testing_y = testing[\"Target\"].as_matrix()\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, nb_epoch=20,batch_size=100, verbose=1, validation_data=(testing_X, testing_y), callbacks=[earlyStopping,modelCheck])))\n",
    "pipeline_keras_nnet = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:46:37.723000",
     "start_time": "2016-11-20T15:46:06.661000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_keras_nnet.fit(training_X,training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:46:48.640000",
     "start_time": "2016-11-20T15:46:45.642000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_nnet= pipeline_keras_nnet.predict(testing[imp_vars])\n",
    "pred_nnet_proba = pipeline_keras_nnet.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_nnet_proba_1 = list(pd.DataFrame(pred_nnet_proba)[1])\n",
    "\n",
    "\n",
    "#liczymy macierz błędu dla nnet\n",
    "\n",
    "cf_nnet=confusion_matrix(testing[\"Target\"],pred_nnet)\n",
    "\n",
    "cf_nnet_pd = pd.DataFrame(cf_nnet)\n",
    "cf_nnet_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_nnet_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "cf_nnet_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu nnet (Keras/Theano)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sens = round(cf_nnet[1][1]*1.00 / np.sum(cf_nnet[1]),2)\n",
    "spec = round(cf_nnet[0][0]*1.00 / np.sum(cf_nnet[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_nnet_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:47:06.338000",
     "start_time": "2016-11-20T15:47:06.298000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_nnet, tpr_nnet, thresholds_nnet = roc_curve(testing[\"Target\"], pred_nnet_proba_1, pos_label=1)\n",
    "\n",
    "roc_auc_nnet = auc(fpr_nnet, tpr_nnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:47:07.840000",
     "start_time": "2016-11-20T15:47:07.581000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkgreen',\n",
    "         lw=lw, label='Krzywa ROC reg.logistyczna (pod krzywa = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label='Krzywa ROC xgboost (pod krzywa = %0.2f)' % roc_auc_xgb, linestyle='--')\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkviolet',\n",
    "         lw=lw, label='Krzywa ROC svm (pod krzywa = %0.2f)' % roc_auc_svm, linestyle=':')\n",
    "plt.plot(fpr_nnet, tpr_nnet, color='darkblue',\n",
    "         lw=lw, label='Krzywa ROC nnet (pod krzywa = %0.2f)' % roc_auc_nnet, linestyle=':')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Falszywe alarmy')\n",
    "plt.ylabel('Pozytywne wykrycia')\n",
    "plt.title('Krzywe ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Można skorzystać z kilku modeli na raz :) \n",
    "\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:47:47.323000",
     "start_time": "2016-11-20T15:47:47.315000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#krok1 - definiujemy modele\n",
    "\n",
    "clf1 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=1e-4, reg_lambda=1e-4,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
    "\n",
    "clf2 = ExtraTreesClassifier(n_estimators=300, criterion='gini', max_depth=20, min_samples_split=20, min_samples_leaf=15, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=True, n_jobs=-1, random_state=None, verbose=1, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:47:47.987000",
     "start_time": "2016-11-20T15:47:47.982000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#krok 2 - korzystamy z VotingClassifier w sklearn\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('xgboost', clf1),('lr',clf2)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:47:51.324000",
     "start_time": "2016-11-20T15:47:51.318000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators_eclf = []\n",
    "estimators_eclf.append(('standardize', StandardScaler()))\n",
    "estimators_eclf.append(('eclf', eclf))\n",
    "pipeline_eclf = Pipeline(estimators_eclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:48:15.783000",
     "start_time": "2016-11-20T15:47:54.302000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_eclf.fit(training_X,training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:48:26.330000",
     "start_time": "2016-11-20T15:48:22.472000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#sprawdźmy jakość naszego modelu na danych testowych\n",
    "\n",
    "pred_eclf= pipeline_eclf.predict(testing[imp_vars])\n",
    "pred_eclf_proba = pipeline_eclf.predict_proba(testing[imp_vars])\n",
    "\n",
    "pred_eclf_proba_1 = list(pd.DataFrame(pred_eclf_proba)[1])\n",
    "\n",
    "\n",
    "#liczymy macierz błędu dla ensemble\n",
    "\n",
    "cf_eclf=confusion_matrix(testing[\"Target\"],pred_eclf)\n",
    "\n",
    "cf_eclf_pd = pd.DataFrame(cf_eclf)\n",
    "cf_eclf_pd.columns=[\"predykcja 0\",\"predykcja 1\"]\n",
    "cf_eclf_pd.index = [\"realne 0\",\"realne 1\"]\n",
    "cf_eclf_pd.plot(kind=\"bar\",title=\"predykcja konwersji wg modelu Ensemble\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sens = round(cf_eclf[1][1]*1.00 / np.sum(cf_eclf[1]),2)\n",
    "spec = round(cf_eclf[0][0]*1.00 / np.sum(cf_eclf[0]),2)\n",
    "aucc = round(roc_auc_score(y_true=testing[\"Target\"],y_score=pred_eclf_proba_1),2)\n",
    "\n",
    "print(\"czułość: \"+str(sens))\n",
    "print(\"specyficzność: \"+str(spec))\n",
    "print(\"powierzchnia pod krzywą:\"+str(aucc))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:48:31.768000",
     "start_time": "2016-11-20T15:48:31.729000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr_eclf, tpr_eclf, thresholds_eclf = roc_curve(testing[\"Target\"], pred_eclf_proba_1, pos_label=1)\n",
    "\n",
    "roc_auc_eclf = auc(fpr_eclf, tpr_eclf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:48:35.150000",
     "start_time": "2016-11-20T15:48:34.902000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_nnet, tpr_nnet, color='darkgreen',\n",
    "         lw=lw, label='Krzywa ROC nnet (pod krzywa = %0.2f)' % roc_auc_nnet, linestyle=':')\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='red',\n",
    "         lw=lw, label='Krzywa ROC xgboost (pod krzywa = %0.2f)' % roc_auc_xgb, linestyle='-.')\n",
    "plt.plot(fpr_eclf, tpr_eclf, color='darkblue',\n",
    "         lw=lw, label='Krzywa ROC Ensemble (pod krzywa = %0.2f)' % roc_auc_eclf, linestyle='--')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Falszywe alarmy')\n",
    "plt.ylabel('Pozytywne wykrycia')\n",
    "plt.title('Krzywe ROC dla wielu modeli')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sprawdźmy na koniec korelacje między wynikami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:50:37.701000",
     "start_time": "2016-11-20T15:50:37.600000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_frame = pd.DataFrame()\n",
    "\n",
    "pred_frame[\"ensemble\"] = list(pred_eclf_proba_1)\n",
    "pred_frame[\"xgboost\"] = list(pred_xgb_proba_1)\n",
    "pred_frame[\"nnet\"] = list(pred_nnet_proba_1)\n",
    "\n",
    "print(\"korelacje między modelami predykcyjnymi:\")\n",
    "print(str(pred_frame.corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:50:56.807000",
     "start_time": "2016-11-20T15:50:51.560000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(pred_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybranie i wdrożenie finalnego modelu na zbiorze dataset_apply zostawiam dla chętnych :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dziękuję za udział w warsztatach, chętnie wysłucham Państwa uwag i pytań :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literatura warta uwagi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:17:24.936000",
     "start_time": "2016-11-20T15:17:24.929000"
    }
   },
   "source": [
    "<img src=\"book1.jpg\", width=400,height=400>\n",
    "<img src=\"book2.jpg\", width=400,height=400>\n",
    "<img src=\"book3.png\", width=400,height=400>\n",
    "<img src=\"book4.jpg\", width=400,height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
